{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aac3d914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For ease of use, the mpas-analysis conda environment can be installed, \n",
    "# which includes most of the following packages.\n",
    "\n",
    "# xeofs, rioxarray will be required as additional packages.\n",
    "# cmocean colormaps may be required, if plotting the EOF mode figures.\n",
    "\n",
    "import sys\n",
    "import os\n",
    "os.environ['USE_PYGEOS'] = '0'\n",
    "import gc\n",
    "import collections\n",
    "\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib import rcParams, cycler\n",
    "from matplotlib import animation, rc\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "from xeofs.xarray import EOF\n",
    "import rioxarray\n",
    "\n",
    "import scipy\n",
    "from scipy import signal\n",
    "from shapely.geometry import mapping\n",
    "from xarrayutils.utils import linear_trend, xr_linregress\n",
    "import pandas as pd\n",
    "#import cmocean\n",
    "\n",
    "# NOTE:\n",
    "# This version of the code uses a modified version of the xeofs package which can be downloaded here:\n",
    "# https://github.com/mshiv/xeofs-rand\n",
    "# Future versions with refactored codebase can be found at https://github.com/mshiv/aislens_emulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77cbe269",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: b'/main_directory_path/RegriddedFluxSSH.nc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/opt/anaconda3/envs/aislens/lib/python3.10/site-packages/xarray/backends/file_manager.py:209\u001b[0m, in \u001b[0;36mCachingFileManager._acquire_with_cache_info\u001b[0;34m(self, needs_lock)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 209\u001b[0m     file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cache\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_key\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/aislens/lib/python3.10/site-packages/xarray/backends/lru_cache.py:55\u001b[0m, in \u001b[0;36mLRUCache.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m---> 55\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cache\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cache\u001b[38;5;241m.\u001b[39mmove_to_end(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: [<class 'netCDF4._netCDF4.Dataset'>, ('/main_directory_path/RegriddedFluxSSH.nc',), 'r', (('clobber', True), ('diskless', False), ('format', 'NETCDF4'), ('persist', False)), 'c0ac6338-b77d-4264-a9a9-01ba543d9ab6']",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 19\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# DATA FILE PATHS\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Data containing regridded flux and SSH for 150 years\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# The input dataset is a 3 dimensional NETCDF file with ('time','y','x') dimensions.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# so the MPAS-Ocean dataset should be regridded. These steps are provided in separate Python scripts,\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# which assume the user has access to MPAS-Ocean output.\u001b[39;00m\n\u001b[1;32m     18\u001b[0m regriddedFluxSSH_filepath \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRegriddedFluxSSH.nc\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 19\u001b[0m regridded_data \u001b[38;5;241m=\u001b[39m \u001b[43mxr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43minDirName\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mregriddedFluxSSH_filepath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m flux \u001b[38;5;241m=\u001b[39m regridded_data\u001b[38;5;241m.\u001b[39mtimeMonthly_avg_landIceFreshwaterFlux\n\u001b[1;32m     21\u001b[0m ssh \u001b[38;5;241m=\u001b[39m regridded_data\u001b[38;5;241m.\u001b[39mtimeMonthly_avg_ssh\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/aislens/lib/python3.10/site-packages/xarray/backends/api.py:541\u001b[0m, in \u001b[0;36mopen_dataset\u001b[0;34m(filename_or_obj, engine, chunks, cache, decode_cf, mask_and_scale, decode_times, decode_timedelta, use_cftime, concat_characters, decode_coords, drop_variables, inline_array, backend_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m    529\u001b[0m decoders \u001b[38;5;241m=\u001b[39m _resolve_decoders_kwargs(\n\u001b[1;32m    530\u001b[0m     decode_cf,\n\u001b[1;32m    531\u001b[0m     open_backend_dataset_parameters\u001b[38;5;241m=\u001b[39mbackend\u001b[38;5;241m.\u001b[39mopen_dataset_parameters,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    537\u001b[0m     decode_coords\u001b[38;5;241m=\u001b[39mdecode_coords,\n\u001b[1;32m    538\u001b[0m )\n\u001b[1;32m    540\u001b[0m overwrite_encoded_chunks \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverwrite_encoded_chunks\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 541\u001b[0m backend_ds \u001b[38;5;241m=\u001b[39m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    542\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    543\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdrop_variables\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdrop_variables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    544\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdecoders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    545\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    546\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    547\u001b[0m ds \u001b[38;5;241m=\u001b[39m _dataset_from_backend_dataset(\n\u001b[1;32m    548\u001b[0m     backend_ds,\n\u001b[1;32m    549\u001b[0m     filename_or_obj,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    557\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    558\u001b[0m )\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ds\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/aislens/lib/python3.10/site-packages/xarray/backends/netCDF4_.py:578\u001b[0m, in \u001b[0;36mNetCDF4BackendEntrypoint.open_dataset\u001b[0;34m(self, filename_or_obj, mask_and_scale, decode_times, concat_characters, decode_coords, drop_variables, use_cftime, decode_timedelta, group, mode, format, clobber, diskless, persist, lock, autoclose)\u001b[0m\n\u001b[1;32m    557\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mopen_dataset\u001b[39m(\n\u001b[1;32m    558\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    559\u001b[0m     filename_or_obj,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    574\u001b[0m     autoclose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    575\u001b[0m ):\n\u001b[1;32m    577\u001b[0m     filename_or_obj \u001b[38;5;241m=\u001b[39m _normalize_path(filename_or_obj)\n\u001b[0;32m--> 578\u001b[0m     store \u001b[38;5;241m=\u001b[39m \u001b[43mNetCDF4DataStore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    580\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    581\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclobber\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclobber\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    584\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdiskless\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdiskless\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    585\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpersist\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpersist\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    587\u001b[0m \u001b[43m        \u001b[49m\u001b[43mautoclose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mautoclose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    588\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    590\u001b[0m     store_entrypoint \u001b[38;5;241m=\u001b[39m StoreBackendEntrypoint()\n\u001b[1;32m    591\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m close_on_error(store):\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/aislens/lib/python3.10/site-packages/xarray/backends/netCDF4_.py:382\u001b[0m, in \u001b[0;36mNetCDF4DataStore.open\u001b[0;34m(cls, filename, mode, format, group, clobber, diskless, persist, lock, lock_maker, autoclose)\u001b[0m\n\u001b[1;32m    376\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[1;32m    377\u001b[0m     clobber\u001b[38;5;241m=\u001b[39mclobber, diskless\u001b[38;5;241m=\u001b[39mdiskless, persist\u001b[38;5;241m=\u001b[39mpersist, \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mformat\u001b[39m\n\u001b[1;32m    378\u001b[0m )\n\u001b[1;32m    379\u001b[0m manager \u001b[38;5;241m=\u001b[39m CachingFileManager(\n\u001b[1;32m    380\u001b[0m     netCDF4\u001b[38;5;241m.\u001b[39mDataset, filename, mode\u001b[38;5;241m=\u001b[39mmode, kwargs\u001b[38;5;241m=\u001b[39mkwargs\n\u001b[1;32m    381\u001b[0m )\n\u001b[0;32m--> 382\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmanager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mautoclose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mautoclose\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/aislens/lib/python3.10/site-packages/xarray/backends/netCDF4_.py:329\u001b[0m, in \u001b[0;36mNetCDF4DataStore.__init__\u001b[0;34m(self, manager, group, mode, lock, autoclose)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_group \u001b[38;5;241m=\u001b[39m group\n\u001b[1;32m    328\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode \u001b[38;5;241m=\u001b[39m mode\n\u001b[0;32m--> 329\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mds\u001b[49m\u001b[38;5;241m.\u001b[39mdata_model\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mds\u001b[38;5;241m.\u001b[39mfilepath()\n\u001b[1;32m    331\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_remote \u001b[38;5;241m=\u001b[39m is_remote_uri(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_filename)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/aislens/lib/python3.10/site-packages/xarray/backends/netCDF4_.py:391\u001b[0m, in \u001b[0;36mNetCDF4DataStore.ds\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    390\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mds\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 391\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_acquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/aislens/lib/python3.10/site-packages/xarray/backends/netCDF4_.py:385\u001b[0m, in \u001b[0;36mNetCDF4DataStore._acquire\u001b[0;34m(self, needs_lock)\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_acquire\u001b[39m(\u001b[38;5;28mself\u001b[39m, needs_lock\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m--> 385\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_manager\u001b[38;5;241m.\u001b[39macquire_context(needs_lock) \u001b[38;5;28;01mas\u001b[39;00m root:\n\u001b[1;32m    386\u001b[0m         ds \u001b[38;5;241m=\u001b[39m _nc4_require_group(root, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_group, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode)\n\u001b[1;32m    387\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ds\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/aislens/lib/python3.10/contextlib.py:135\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerator didn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt yield\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/aislens/lib/python3.10/site-packages/xarray/backends/file_manager.py:197\u001b[0m, in \u001b[0;36mCachingFileManager.acquire_context\u001b[0;34m(self, needs_lock)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;129m@contextlib\u001b[39m\u001b[38;5;241m.\u001b[39mcontextmanager\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21macquire_context\u001b[39m(\u001b[38;5;28mself\u001b[39m, needs_lock\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    196\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Context manager for acquiring a file.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m     file, cached \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_acquire_with_cache_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43mneeds_lock\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    199\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m file\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/aislens/lib/python3.10/site-packages/xarray/backends/file_manager.py:215\u001b[0m, in \u001b[0;36mCachingFileManager._acquire_with_cache_info\u001b[0;34m(self, needs_lock)\u001b[0m\n\u001b[1;32m    213\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m    214\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode\n\u001b[0;32m--> 215\u001b[0m file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_opener\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# ensure file doesn't get overridden when opened again\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32msrc/netCDF4/_netCDF4.pyx:2463\u001b[0m, in \u001b[0;36mnetCDF4._netCDF4.Dataset.__init__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/netCDF4/_netCDF4.pyx:2026\u001b[0m, in \u001b[0;36mnetCDF4._netCDF4._ensure_nc_success\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: b'/main_directory_path/RegriddedFluxSSH.nc'"
     ]
    }
   ],
   "source": [
    "# To update following with relative repository paths once data and code is up on Zenodo\n",
    "# Current version uses the project template on Github.\n",
    "\n",
    "# Define project repo path\n",
    "inDirName = '/main_directory_path/'\n",
    "\n",
    "# DATA FILE PATHS\n",
    "# Data containing regridded flux and SSH for 150 years\n",
    "# The input dataset is a 3 dimensional NETCDF file with ('time','y','x') dimensions.\n",
    "# 'y','x' are used here as the data is gridded in polar stereographic coordinates.\n",
    "\n",
    "# The variable resolution dataset used in this study can be downloaded from\n",
    "# https://esgf-node.llnl.gov/search/e3sm/ (for V1 Cryosphere Campaign)\n",
    "# The current workflow in this notebook is set up for a constant resolution grid in space and time, \n",
    "# so the MPAS-Ocean dataset should be regridded. These steps are provided in separate Python scripts,\n",
    "# which assume the user has access to MPAS-Ocean output.\n",
    "\n",
    "regriddedFluxSSH_filepath = 'RegriddedFluxSSH.nc'\n",
    "regridded_data = xr.open_dataset(inDirName+regriddedFluxSSH_filepath)\n",
    "flux = regridded_data.timeMonthly_avg_landIceFreshwaterFlux\n",
    "ssh = regridded_data.timeMonthly_avg_ssh\n",
    "\n",
    "# Ice shelf boundary definitions: https://github.com/MPAS-Dev/geometric_features\n",
    "iceShelvesShape_filepath = 'iceShelves.geojson'\n",
    "\n",
    "# \"Dedrafted \"\n",
    "iceshelves_dedrafted_total = xr.open_dataset('iceshelves_dedrafted_total.nc')\n",
    "flux_dedraft = iceshelves_dedrafted_total.timeMonthly_avg_landIceFreshwaterFlux\n",
    "\n",
    "# Demean: remove temporal mean\n",
    "flux_dedraft_tmean = flux_dedraft.mean('time')\n",
    "flux_dedraft_demeaned = flux_dedraft - flux_dedraft_tmean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d4f189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original forcing data: raw MPAS-O output, mapped to the 10km resolution grid\n",
    "# flux is freshwater flux\n",
    "# ssh is sea surface height, used here as a proxy for ice draft / depth.\n",
    "\n",
    "# ssh is used to plot the scatterplot of flux vs. draft for different ice shelves \n",
    "# and as input for the linear regression used in \"dedrafting\" the dataset\n",
    "\n",
    "sec_per_year = 365*24*60*60\n",
    "rho_fw = 1000\n",
    "\n",
    "# Pre-processed data: detrended, deseasonalized, dedrafted\n",
    "flux_clean = xr.open_dataset(inDirName+'flux_clean')\n",
    "flux_clean = flux_clean.timeMonthly_avg_landIceFreshwaterFlux\n",
    "\n",
    "# Flux datapoints extracted for individual ice shelves, used for the scatter plots\n",
    "catchments_scatter = np.load(inDirName+interim_data_folder+\"catchments_scatter.npy\")\n",
    "catchments_scatter_xr = xr.DataArray(catchments_scatter,dims={'basin','x','y'})\n",
    "catchments_scatter = catchments_scatter*sec_per_year/rho_fw\n",
    "\n",
    "# Catchment boundary masks for Antarctica, taken from ice shelf definitions in MPAS-Dev/geometric-features:\n",
    "# https://github.com/MPAS-Dev/geometric_features/tree/main/geometric_data\n",
    "# These have been combined into one file with 133 defined regions (polygons and multipolygons), \n",
    "# readable via the Geopandas package\n",
    "\n",
    "# Read geoJSON region feature file as a GeoDataFrame\n",
    "iceshelvesmask = gpd.read_file(inDirName + 'iceShelves.geojson')\n",
    "\n",
    "# Convert to south polar stereographic projection\n",
    "#icems = iceshelvesmask.to_crs({'init': 'epsg:3031'}); # This has been deprecated\n",
    "icems = iceshelvesmask.to_crs('epsg:3031');\n",
    "crs = ccrs.SouthPolarStereo();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c74213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot boundaries\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "ax1 = plt.subplot(121,projection=ccrs.SouthPolarStereo())\n",
    "#ax1.gridlines(color='whitesmoke',zorder=4)\n",
    "icems[34:133].plot(ax=ax1,color='antiquewhite', linewidth=0,zorder=1)\n",
    "ax1.patch.set_facecolor(color='lightsteelblue')\n",
    "#ax1.add_feature(cartopy.feature.LAND, color='ghostwhite')\n",
    "ax1.add_feature(cartopy.feature.LAND, color='ghostwhite', zorder=2)\n",
    "plt.title('Catchment Boundaries');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce4a2e1",
   "metadata": {},
   "source": [
    "### Sample data for specific ice shelves (time averaged melt rate draft dependence)\n",
    "\n",
    "Scatterplot of Antarctic catchments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a530e4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure: Melt rate draft dependence for select catchments\n",
    "# The following indices are taken from 'catchment_scatter', defined as\n",
    "# Index of catchments in catchment_scatter = Index of catchments in icems - 33\n",
    "\n",
    "# Sample ice shelves chosen:\n",
    "# Amery = 1\n",
    "# George VI = 27\n",
    "# Getz = 28\n",
    "# Pine Island = 61\n",
    "# Ronne = 70\n",
    "# Totten = 83\n",
    "# Wilkins = 94\n",
    "# Western Ross = 71\n",
    "# Filchner = 22\n",
    "# Thwaites = 81\n",
    "# Eastern_Ross = 72\n",
    "\n",
    "# Full list of ice shelves and indices can be obtained by printing icems\n",
    "\n",
    "#catchments = np.array([1,27,28,61,70,83,94,71,22])\n",
    "\n",
    "catchments = np.array([1,27,28,61,70,83])\n",
    "basins=catchments+33\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "#fig, ax = plt.subplots(1, 1, figsize=[15, 15],subplot_kw={'projection': ccrs.SouthPolarStereo()})\n",
    "fig, axs = plt.subplots(2,3, figsize=[15, 10])\n",
    "axs=axs.ravel()\n",
    "\n",
    "for i,label in enumerate(('(a)', '(b)', '(c)', '(d)', '(e)', '(f)')):\n",
    "    axs[i].scatter(catchments_scatter[catchments[i]],ssh[1],color='r',s=1)\n",
    "    #axs[i].set_xlabel('Melt Flux (kg / m2 / s)')\n",
    "    #axs[i].set_ylabel('Depth (m)')\n",
    "    axs[i].set_ylim(-1000,0) # Specified for consistency across plots, some catchments are not as deep as others\n",
    "    # Uncomment to set xlim values for easy comparison of forcing magnitudes (varies across ice shelves)\n",
    "    #axs[i].set_xlim(-0.5e-6,2e-5)\n",
    "    axs[i].set_title('{}'.format(icems.name[33+catchments[i]]))\n",
    "    axs[i].text(-0.04, 1.08, label, transform=axs[i].transAxes ,fontweight='bold', va='top')\n",
    "    # axs[i].inset_axes([], projection=crs) does not seem to work, hence the workaround below:\n",
    "    # Refer here: https://github.com/matplotlib/matplotlib/pull/22608 for future revisions\n",
    "    axins = inset_axes(axs[i],width=\"30%\", height=\"30%\", loc=\"upper right\", \n",
    "                       axes_class=cartopy.mpl.geoaxes.GeoAxes, \n",
    "                       axes_kwargs=dict(projection=ccrs.SouthPolarStereo()))\n",
    "    icems[33:133].plot(ax=axins,linewidth=0.3,color='lightsteelblue')\n",
    "    axins.coastlines(resolution='10m',linewidth=0.25)\n",
    "    icems.loc[[33+catchments[i]],'geometry'].plot(ax=axins,color='r',linewidth=0.4)\n",
    "    #axs[i].set_axis_off();\n",
    "    axins.set_axis_off();\n",
    "    axs[i].grid(alpha=0.5)\n",
    "\n",
    "axs[0].set_ylabel('Depth (m)');\n",
    "axs[3].set_ylabel('Depth (m)');\n",
    "\n",
    "axs[3].set_xlabel('Melt Rate ($m/yr$)');\n",
    "axs[4].set_xlabel('Melt Rate ($m/yr$)');\n",
    "axs[5].set_xlabel('Melt Rate ($m/yr$)');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5d52bf",
   "metadata": {},
   "source": [
    "# Data Pre-Processing\n",
    "\n",
    "For time series plots and comparisons, the sum of flux across the spatial domain (or specific ice shelves) is used as a proxy when required. The mean can also be used here, without any loss of information, as the idea is only to compare trendlines.\n",
    "\n",
    "However, this might not be possible once the data is normalized, as the magnitude of data is removed at that stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae42d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demean: remove temporal mean\n",
    "# A pre-processed (\"dedrafted\") dataset is provided with this code. \n",
    "# In case of a linear regression, the xarray.polyfit or xarrayutils functions can be used:\n",
    "# https://docs.xarray.dev/en/stable/generated/xarray.Dataset.polyfit.html\n",
    "# https://xarrayutils.readthedocs.io/en/latest/utils.html\n",
    "\n",
    "flux_dedraft_tmean = flux_dedraft.mean('time')\n",
    "flux_dedraft_demeaned = flux_dedraft - flux_dedraft_tmean\n",
    "\n",
    "# Deseasonalize\n",
    "# Remove climatologies to isolate anomalies / deseasonalize \n",
    "flux_month = flux_dedraft_demeaned.groupby(\"time.month\")\n",
    "flux_clm = flux_month.mean(\"time\") # Climatologies\n",
    "flux_anm = flux_month - flux_clm # Deseasonalized anomalies\n",
    "\n",
    "# Remove initial 'model spinup period' data before analysis\n",
    "spinup_period_years = 25 # Verify period of datapoints to be removed before PCA\n",
    "spinup_time_period = 12*spinup_period_years\n",
    "#flux_clean = flux_anm[spinup_time_period:]\n",
    "\n",
    "flux_seasonal = (flux_dedraft - flux_anm)*sec_per_year/rho_fw\n",
    "flux_drft = (flux - flux_dedraft)*sec_per_year/rho_fw\n",
    "\n",
    "flux_seasonal = flux_seasonal[spinup_time_period:]\n",
    "flux_drft = flux_drft[spinup_time_period:]\n",
    "\n",
    "# Normalize \n",
    "flux_clean_tmean = flux_clean.mean('time')\n",
    "flux_clean_tstd = flux_clean.std('time')\n",
    "flux_clean_demeaned = flux_clean - flux_clean_tmean\n",
    "flux_clean_normalized = flux_clean_demeaned/flux_clean_tstd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a22324",
   "metadata": {},
   "source": [
    "## EOF/PCA Decomposition\n",
    "\n",
    "EOF decomposition in this version is done using a forked version of the `xeofs` package.\n",
    "Refer [nicrie/xeofs](https://github.com/nicrie/xeofs) on Github for documentation. The forked version, important for reconstructing datasets using the randomized PCs and for using PCA alternatives can be found here: [mshiv/xeofs-rand](https://github.com/mshiv/xeofs-rand). Alternate solvers (PCA, SparsePCA, TruncatedSVD etc. have been implemented as methods, but can also directly be called using the `scikit-learn` package.\n",
    "\n",
    "See [scikit-learn.decomposition](https://scikit-learn.org/stable/modules/decomposition.html#decompositions) user guide for more on usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e75a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Non-normalized variability decomposition \n",
    "model = EOF(flux_clean)\n",
    "model.solve()\n",
    "eofs = model.eofs()\n",
    "pcs = model.pcs()\n",
    "nmodes = model.n_modes\n",
    "varexpl = model.explained_variance_ratio()\n",
    "pcs_eig = model.pcs(1)\n",
    "\n",
    "# Normalized variability decomposition \n",
    "norm_model = EOF(flux_clean_normalized)\n",
    "norm_model.solve()\n",
    "norm_eofs = norm_model.eofs()\n",
    "norm_pcs = norm_model.pcs()\n",
    "norm_nmodes = norm_model.n_modes\n",
    "norm_varexpl = norm_model.explained_variance_ratio()\n",
    "norm_pcs_eig = norm_model.pcs(1)\n",
    "norm_eofs_eig = norm_model.eofs(1)\n",
    "norm_eofs_sng = norm_model.eofs(2)\n",
    "norm_pcs_sng = norm_model.pcs(2)\n",
    "norm_varexpl_values = norm_model.explained_variance()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b93942",
   "metadata": {},
   "source": [
    "### Relative power of eigenmodes and dominant eigenmodes\n",
    "\n",
    "* Model output variability data (`flux_clean`)\n",
    "* Normalized model output variability data (`flux_clean_normalized`) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ff1ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cumulative variance captured\n",
    "Fv_cs = varexpl.cumsum()\n",
    "Fvn_cs = norm_varexpl.cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88e1d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot figure\n",
    "# Figure: EOF modes from decomposition of normalized model variability data\n",
    "\n",
    "nmodes_plot = 3 # Number of modes to plot\n",
    "\n",
    "yr_length = range(0,125)\n",
    "yrs_plot = np.repeat(yr_length, 12)\n",
    "\n",
    "#sns.set_theme()\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "fig=plt.figure(figsize=(12,8))\n",
    "gs = GridSpec(nmodes_plot,2,width_ratios=[1.5,2])\n",
    "axbig = fig.add_subplot(gs[0,:])\n",
    "ax0 = [fig.add_subplot(gs[i,0],projection=crs) for i in range(1,nmodes_plot)]\n",
    "ax1 = [fig.add_subplot(gs[i,1]) for i in range(1,nmodes_plot)]\n",
    "\n",
    "for i, (a0, a1) in enumerate(zip(ax0, ax1)):\n",
    "    norm_eofs_eig.sel(mode=i+1).plot(ax=a0,\n",
    "                            cmap='cmo.balance',\n",
    "                            add_colorbar=True,\n",
    "                            cbar_kwargs={'orientation': 'vertical',\n",
    "                                         'label': 'SD Units'})\n",
    "    a0.coastlines(resolution='10m',linewidth=0.25,color='0.8')\n",
    "    a0.set_title('EOF Mode {}'.format(i+1))\n",
    "    a1.set_ylim(-3,3)\n",
    "    a1.plot(norm_pcs_eig.sel(mode=i+1),linewidth=1,color='dimgray')\n",
    "    a1.set_xlabel('')\n",
    "    a1.set_title('PC Mode {}'.format(i+1))\n",
    "    a1.tick_params(labelbottom=False, left=False, grid_alpha=0.3)\n",
    "\n",
    "a1.set_xlabel('Time (months)');\n",
    "\n",
    "m = 500\n",
    "xvar = np.linspace(1,m,m)\n",
    "axbig.plot(xvar,Fv_cs[:m]*100, 'k*--', lw=0.5,markersize=1.75,\n",
    "         label='Actual Data: {:.2f}\\%'.format(F_v_varexpl[:m].sum().values*100))\n",
    "axbig.plot(xvar,Fvn_cs[:m]*100, 'rh--', lw=0.5,markersize=0.75,\n",
    "         label='Normalized Data: {:.2f}\\%'.format(F_vn_varexpl[:m].sum().values*100))\n",
    "axbig.set_ylabel('Cumulative variance captured (\\%)')\n",
    "axbig.set_xlabel('Mode Number')\n",
    "axbig.set_title('Cumulative variance captured by first {} modes'.format(m));\n",
    "axbig.tick_params(labelbottom=True, left=False, grid_alpha=0.3)\n",
    "axbig.legend();\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7022dcc",
   "metadata": {},
   "source": [
    "## Fourier Phase Randomization\n",
    "\n",
    "Now, the projection co-efficients from the previous step are phase-randomized to generate new realizations of the data. In this step, we sample phases for each FFT(PC) from (0,$2\\pi$), and iterate to generate n realizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d3590d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "##############################\n",
    "# FOURIER PHASE RANDOMIZATION \n",
    "##############################\n",
    "\n",
    "# Define number of random Fourier realizations\n",
    "n_realizations = 5\n",
    "t_length = norm_pcs.shape[0]\n",
    "\n",
    "# Define random number generator \n",
    "#rng = np.random.default_rng(2021)\n",
    "#random_phases = np.exp(np.random.default_rng(2023).uniform(0,2*np.pi,int(len(fl)/2+1))*1.0j) in line 26\n",
    "\n",
    "# xeofs_pcs[:,i] when using PCA outputs\n",
    "new_fl = np.empty((n_realizations,norm_pcs.shape[0],norm_pcs.shape[1]))\n",
    "\n",
    "# Time limits for plotting\n",
    "t1 = 0\n",
    "tf = int(t_length/2)\n",
    "\n",
    "for i in range(n_realizations):\n",
    "    for m in range(nmodes):\n",
    "        fl = norm_pcs[:,m] # fluxpcs[:,i] when using PCA outputs\n",
    "        fl_fourier = np.fft.rfft(fl)\n",
    "        random_phases = np.exp(np.random.uniform(0,2*np.pi,int(len(fl)/2+1))*1.0j)\n",
    "        fl_fourier_new = fl_fourier*random_phases\n",
    "        new_fl[i,:,m] = np.fft.irfft(fl_fourier_new)\n",
    "    print('calculated ifft for realization {}, all modes'.format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ce634d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagnostic plot of generated PCs\n",
    "\n",
    "nmodes_plot = 4 # Number of modes to plot\n",
    "nrealizations_to_plot = 5 # to be lesser than absolute total number, defined in the Fourier randomization step\n",
    "\n",
    "sns.set_theme(style=\"white\")\n",
    "fig=plt.figure(figsize=(25,20))\n",
    "\n",
    "gs = GridSpec(nmodes_plot, 2, width_ratios=[4, 2])\n",
    "ax0 = [fig.add_subplot(gs[i, 0]) for i in range(nmodes_plot)]\n",
    "ax1 = [fig.add_subplot(gs[i, 1]) for i in range(nmodes_plot)]\n",
    "\n",
    "for i, (a0,a1) in enumerate(zip(ax0,ax1)):\n",
    "    for n_realization in range(0,nrealizations_to_plot):\n",
    "        a0.plot(new_fl[n_realization,:,i],color='b', linewidth=0.5)\n",
    "        a1.psd(new_fl[n_realization,:,i],color='b', linewidth=0.5)\n",
    "    a0.plot(new_fl[0,:,i],color='b', linewidth=0.25,label='Randomized')\n",
    "    a1.psd(new_fl[0,:,i],color='b', linewidth=0.25,label='Randomized')\n",
    "    a0.plot(pcs[:,i],color='k', linewidth=2.5,label='Original')\n",
    "    a1.psd(pcs[:,i],color='k', linewidth=2.5,label='Original')\n",
    "    a0.set_title('PC for EOF mode {}'.format(i+1))\n",
    "    a1.set_title('PSD for PC mode {}'.format(i+1))\n",
    "    a1.set_xlabel('')\n",
    "\n",
    "a0.set_xlabel('Time (months)')\n",
    "a1.set_xlabel('Frequency')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0fb0cb3",
   "metadata": {},
   "source": [
    "### Helper Functions\n",
    "\n",
    "Functions defined for the generation and follow on calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3e60f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#modified to not return f - in calculation of RMSE, only Px required\n",
    "def psd_calc_grid(data,y,x):\n",
    "    f, Px = scipy.signal.welch(data[:,y,x])\n",
    "    return Px\n",
    "\n",
    "def time_series(clipped_data):\n",
    "    clipped_ts = clipped_data.sum(['y','x'])\n",
    "    return clipped_ts\n",
    "\n",
    "# Reconstruct flux dataset using phase randomized PCs\n",
    "# This section is to be called iteratively for ensemble runs with multiple realizations\n",
    "# This method also takes 'modes' as a parameter: \n",
    "# which is used to reconstruct dataset with different number of selected modes\n",
    "\n",
    "def generate_data(n_realization,mode,mode_skip):\n",
    "    # mode can be any int in (1,nmodes), for cases \n",
    "    # when dimensionality reduction is preferred on the reconstructed dataset\n",
    "    flux_reconstr = norm_model.reconstruct_randomized_X(new_fl[n_realization],slice(1,mode,mode_skip))\n",
    "    #flux_reconstr = flux_reconstr.dropna('time',how='all')\n",
    "    #flux_reconstr = flux_reconstr.dropna('y',how='all')\n",
    "    #flux_reconstr = flux_reconstr.dropna('x',how='all')\n",
    "    #flux_reconstr = flux_reconstr.drop(\"month\")\n",
    "    return flux_reconstr\n",
    "\n",
    "def clip_data(total_data, basin):\n",
    "    clipped_data = total_data.rio.clip(icems.loc[[basin],'geometry'].apply(mapping))\n",
    "    #clipped_data = clipped_data.dropna('time',how='all')\n",
    "    #clipped_data = clipped_data.dropna('y',how='all')\n",
    "    #clipped_data = clipped_data.dropna('x',how='all')\n",
    "    clipped_data = clipped_data.drop(\"month\")\n",
    "    return clipped_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a885152",
   "metadata": {},
   "source": [
    "### Statistical Generation\n",
    "\n",
    "This step uses functions committed to the `xeofs-rand` fork. *Ref: For commit diff details, see [mshiv/xeofs-rand](https://github.com/nicrie/xeofs/compare/main...mshiv:xeofs-rand:main).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134839a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate dataset realizations\n",
    "\n",
    "## Standard EOF/PCA implementation\n",
    "# Can use the xeofs-rand package, or directly generate using sklearn PCA.\n",
    "\n",
    "for i in range(n_realizations):\n",
    "    flux_reconstr = generate_data(i, 1500, 1)\n",
    "    flux_reconstr = (flux_reconstr*flux_clean_tstd)+flux_clean_tmean\n",
    "    melt_reconstr = flux_reconstr*sec_per_year/rho_fw\n",
    "    melt_reconstr = melt_reconstr.rename('rec{}'.format(n_realizations))\n",
    "    melt_reconstr.to_netcdf(inDirName+\"REC{}.nc\".format(i))\n",
    "    print('reconstructed realization # {}'.format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e912b4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Input generated dataset files\n",
    "# This section presents a sample case of 1 generated realization\n",
    "# Customize this section as necessary\n",
    "rec0 = xr.open_dataset(inDirName+\"REC0.nc\").rec0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0c37ac",
   "metadata": {},
   "source": [
    "### Sparse PCA Implementation\n",
    "\n",
    "[`sklearn.decomposition.SparsePCA`](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.SparsePCA.html) was added to the `xeofs-rand` fork as a method.\n",
    "\n",
    "`MiniBatchSparsePCA` can be used instead to improve computational speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56106d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################\n",
    "# SPARSE DECOMPOSITION \n",
    "##############################\n",
    "\n",
    "norm_model.solvesparse()\n",
    "spca_eofs = model.eofs()\n",
    "spca_pcs = model.pcs()\n",
    "\n",
    "##############################\n",
    "# FOURIER PHASE RANDOMIZATION \n",
    "##############################\n",
    "\n",
    "# Define number of random Fourier realizations\n",
    "n_realizations = 3\n",
    "t_length = pcs.shape[0]\n",
    "\n",
    "# Define random number generator \n",
    "#rng = np.random.default_rng(2021)\n",
    "#random_phases = np.exp(np.random.default_rng(2023).uniform(0,2*np.pi,int(len(fl)/2+1))*1.0j) in line 26\n",
    "\n",
    "# xeofs_pcs[:,i] when using PCA outputs\n",
    "new_fl = np.empty((n_realizations,pcs.shape[0],pcs.shape[1]))\n",
    "\n",
    "# Time limits for plotting\n",
    "t1 = 0\n",
    "tf = int(t_length/2)\n",
    "\n",
    "for i in range(n_realizations):\n",
    "    for m in range(nmodes):\n",
    "        fl = pcs[:,m] # fluxpcs[:,i] when using PCA outputs\n",
    "        fl_fourier = np.fft.rfft(fl)\n",
    "        #random_phases = np.exp(np.random.uniform(0,2*np.pi,int(len(fl)/2+1))*1.0j)\n",
    "        #random_phases = np.exp(np.random.default_rng(2023).uniform(0,2*np.pi,int(len(fl)/2+1))*1.0j)\n",
    "        random_phases = np.exp(np.random.uniform(0,2*np.pi,int(len(fl)/2+1))*1.0j)\n",
    "        fl_fourier_new = fl_fourier*random_phases\n",
    "        new_fl[i,:,m] = np.fft.irfft(fl_fourier_new)\n",
    "    print('calculated ifft for realization {}, all modes'.format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b04c1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n_realizations):\n",
    "    new_fl_xr = xr.DataArray(new_fl[i], dims=pcs.dims, coords = pcs.coords, attrs = pcs.attrs)\n",
    "    sparse_mini_rand = new_fl_xr @ spca_eofs.T\n",
    "    # Imperative to check the above generated dataset for consistent dimensions\n",
    "    sparse_mini_rand = sparse_mini_rand.transpose(\"time\",\"y\",\"x\")\n",
    "    sparse_mini_rand = (sparse_mini_rand*flux_clean_tstd)+flux_clean_tmean\n",
    "    melt_spca_reconstr = sparse_mini_rand*sec_per_year/rho_fw\n",
    "    melt_spca_reconstr = melt_spca_reconstr.rename('spcarec{}'.format(n_realizations))\n",
    "    sparse_mini_rand.to_netcdf(inDirName+\"spcaREC{}.nc\".format(i))\n",
    "    print(\"Created randomized realization...{}\".format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84d8da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Input generated dataset files\n",
    "# This section presents a sample case of 1 generated realization\n",
    "# Customize this section as necessary\n",
    "spcarec0 = xr.open_dataset(inDirName+\"spcaREC0.nc\").spcarec0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380f3c79",
   "metadata": {},
   "source": [
    "### Generator variability output \n",
    "\n",
    "For full AIS, and select basins: time series at a grid point and power spectral densities for the full ice shelf are plotted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc3c93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#catchments = np.array([1,27,28,61,70,83,94,71,22])\n",
    "#basins = catchments+33\n",
    "\n",
    "#basins = [34, 55, 60, 61, 94, 103]\n",
    "#basins = [55,94]\n",
    "#basins = [34, 60]\n",
    "#grid_points = np.array(([85,105], [115,45], [4,45],[21,56], [11,10], [34,95]))\n",
    "#grid_points = np.array(([115,45],[11,10]))\n",
    "#grid_points = np.array(([85,105],[4,45]))\n",
    "\n",
    "basins = [34,94,60,55,94]\n",
    "# Choose grid points based on dataset indices. Can find these using data.x and data.y dimension arrays.\n",
    "# Locations here have been selected based on performance of model simulation data at these points, \n",
    "# given also the variance between them (in terms of temporal scale of variability)\n",
    "grid_points = np.array(([85,105], [11,10],[15,44], [100,40]))\n",
    "\n",
    "#basins = np.array([34, 60, 94, 55])\n",
    "# [2,3,4,5] = [Antarctica, Peninsula, West Antarctica, East Antarctica]\n",
    "\n",
    "n_basins = len(basins)\n",
    "oc = 'k'\n",
    "pcac = '#D55E00'#'#FC8D62'\n",
    "spcac = 'steelblue'#'#0072B2' #'#CC79A7'\n",
    "\n",
    "lwbg = 0.1\n",
    "lworig = 1.5\n",
    "lwgen = 0.8\n",
    "\n",
    "sns.set_theme(style=\"white\")\n",
    "fig=plt.figure(figsize=(18,4*n_basins))\n",
    "gs = GridSpec(n_basins, 2, width_ratios=[1,3])\n",
    "axtop = fig.add_subplot(gs[0, :])\n",
    "ax0 = [fig.add_subplot(gs[i, 0]) for i in range(1,n_basins)]\n",
    "ax1 = [fig.add_subplot(gs[i, 1]) for i in range(1,n_basins)]\n",
    "\n",
    "for i,(a0,a1,grids) in enumerate(zip(ax0,ax1,grid_points)):\n",
    "    orig = clip_data(flux_clean, basins[i])\n",
    "    gen0 = clip_data(rec0, basins[i])\n",
    "    spcagen0 = clip_data(spcarec0, basins[i])\n",
    "    combined = xr.merge([orig,gen0,spcagen0])\n",
    "    combined_ts = time_series(combined)\n",
    "    a1.plot(orig[:,grids[0],grids[1]],color=oc, linewidth=lworig,label='Model Data ($F_v$)')\n",
    "    a1.plot(spcagen0[:,grids[0],grids[1]],color=spcac, linewidth=lwgen,label='Sparse PCA Gen. Data ($F\\'_v$)')\n",
    "    a1.plot(gen0[:,grids[0],grids[1]],color=pcac, linewidth=lwgen, label='PCA Gen. Data ($F\\'_v$)')\n",
    "    axin1 = inset_axes(a1, width=\"30%\", height=\"30%\", loc=\"upper right\", \n",
    "                       axes_class=cartopy.mpl.geoaxes.GeoAxes, \n",
    "                       bbox_to_anchor=(0.07,-0.025,1,1),bbox_transform=a1.transAxes,\n",
    "                       axes_kwargs=dict(projection=ccrs.SouthPolarStereo()))\n",
    "    axin1.coastlines(resolution='10m',linewidth=0.25)\n",
    "    axin1.add_feature(cartopy.feature.LAND, color='#B3B3B3',zorder=2,alpha=0.5)\n",
    "    icems.loc[[basins[i]],'geometry'].plot(ax=axin1,color='#B3B3B3',linewidth=0.4,alpha=0.75,zorder=3)\n",
    "    axin1.coastlines(resolution='10m',linewidth=0.25,zorder=4)\n",
    "    axin1.plot(orig.x[grids[1]],orig.y[grids[0]],marker='*',color='k',zorder=4)\n",
    "    a1.set_title('Sample melt rate variability in the {} catchment'.format(icems.name[basins[i]]))\n",
    "    a1.set_ylabel('Melt Rate ($m/yr$)');\n",
    "    a1.set_xlabel('')\n",
    "    a1.tick_params(labelbottom=False)\n",
    "    a1.legend(loc=\"lower right\");\n",
    "    a1.grid(color='dimgray', linestyle='-', linewidth=0.25)\n",
    "    a0.psd(combined_ts.spcagen0,color=spcac, linewidth=lwgen)\n",
    "    a0.psd(combined_ts.gen0,color=pcac, linewidth=lwgen)\n",
    "    a0.psd(combined_ts.orig,color=oc, linewidth=lworig)    \n",
    "    axin0 = inset_axes(a0, width=\"30%\", height=\"30%\", loc=\"upper right\", \n",
    "                       axes_class=cartopy.mpl.geoaxes.GeoAxes, \n",
    "                       bbox_to_anchor=(-0.99,-0.025,1,1),bbox_transform=a1.transAxes,\n",
    "                       axes_kwargs=dict(projection=ccrs.SouthPolarStereo()))\n",
    "    icems[33:133].plot(ax=axin0,linewidth=0.3,color='#B3B3B3')\n",
    "    axin0.coastlines(resolution='10m',linewidth=0.15)\n",
    "    icems.loc[[basins[i]],'geometry'].plot(ax=axin0,color='k',linewidth=0.4, label=\"Selected Grid Point\")\n",
    "    a0.set_title('Full ice shelf PSD: {}'.format(icems.name[basins[i]]))\n",
    "    a0.tick_params(labelbottom=False)\n",
    "    a0.set_xlabel('')\n",
    "    del orig, gen0, spcagen0, combined, combined_ts\n",
    "a1.set_xlabel('Time (months)');\n",
    "a0.set_xlabel('Frequency');\n",
    "\n",
    "# Full AIS power spectral density plots\n",
    "axtop.psd(time_series(flux_clean), color = oc, lw=lworig,label='Model Data ($F_v$)');\n",
    "axtop.psd(time_series(rec0), color= pcac, lw=lwgen, label='PCA Gen. Data ($F\\'_v$)');\n",
    "axtop.psd(time_series(spcarec0), color= spcac, lw=lwgen,label='Sparse PCA Gen. Data ($F\\'_v$)');\n",
    "axintop = inset_axes(axtop, width=\"30%\", height=\"30%\", loc=\"upper right\", \n",
    "                     bbox_to_anchor=(0.1,-0.025,1,1),bbox_transform=axtop.transAxes,\n",
    "                     axes_class=cartopy.mpl.geoaxes.GeoAxes, \n",
    "                     axes_kwargs=dict(projection=ccrs.SouthPolarStereo()))\n",
    "icems[2:3].plot(ax=axintop,linewidth=0.3,color='k',alpha=1)\n",
    "axintop.coastlines(resolution='10m',linewidth=0.15,color='white',zorder=5)\n",
    "axtop.set_title('Full AIS domain PSD Comparisons');\n",
    "axtop.legend(loc=\"lower right\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7c3dbe",
   "metadata": {},
   "source": [
    "## Generator output with all components added\n",
    "\n",
    "Generator output for a select few grid points in final output, with all components added back (Seasonality+draft dependence)\n",
    "\n",
    "Use the following grid points to plot time series of original data and generated data. Grid point indices are specified for clipped subdomains. (Use `clip_data(data, basin_number)` and use the specified indices on the output).\n",
    "\n",
    "Locations:\n",
    "1. Amery (basin = 34) : `(y=85,x=105)`\n",
    "2. George VI (basin = 60): `(y=4,x=45)`\n",
    "3. Getz (basin = 61): `(y=21,x=58)`\n",
    "4. Ronne (basin = 103): `(y=95,x=34)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d280decc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See notebook 06-sm-... for methods to obtain original dataset, \n",
    "# seasonality calculations and dedrafting datafile locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98bfee92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure all datasets are in terms of melt rate [m/yr], not freshwater flux\n",
    "F = flux*sec_per_year/rho_fw\n",
    "Frec = rec0 + flux_seasonal + flux_drft\n",
    "Fspcarec = spcarec0 + flux_seasonal + flux_drft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3803e906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ORIGINAL DATA\n",
    "#orig = clip_data(flux, basin)\n",
    "#basins = [34, 60, 61, 103]\n",
    "#grid_points = np.array([[85,105],[4,45],[21,56],[95,34]])\n",
    "\n",
    "basins = [34,94]\n",
    "#grid_points = np.array(([85,105], [11,10],[15,44], [100,40]))\n",
    "grid_points = np.array(([85,105], [11,10]))\n",
    "\n",
    "basins = [34,94,60,55]\n",
    "#grid_points = np.array(([85,105], [11,10],[15,44], [100,40]))\n",
    "grid_points = np.array(([85,105], [11,10],[12,41], [100,40]))\n",
    "\n",
    "sns.set_theme(style=\"white\")\n",
    "\n",
    "fig, axs = plt.subplots(4,1, figsize=[20, 20])\n",
    "axs=axs.ravel()\n",
    "\n",
    "pltlabels = ['(a)','(b)','(c)','(d)']\n",
    "\n",
    "#n_basins = len(basins)\n",
    "color_orig = 'k'\n",
    "color_gen = 'dimgray'\n",
    "\n",
    "for i, grids in enumerate(zip(grid_points)):\n",
    "    #ssnl = clip_data(flux_seasonal, basins[i])\n",
    "    #drft = clip_data(flux_drft, basins[i])\n",
    "    orig = clip_data(F, basins[i])\n",
    "    spcarec = clip_data(Fspcarec, basins[i])\n",
    "    axs[i].plot(orig[:,grids[0][0],grids[0][1]],color=oc, linewidth=lworig,label='Model Data ($F_{orig}$)')\n",
    "    axs[i].plot(spcarec[:,grids[0][0],grids[0][1]],color=spcac, linewidth=lwgen,label='Sparse PCA Gen. Data ($F\\'_{gen}$)')\n",
    "    axs[i].set_title('Sample melt rate in the {} catchment'.format(icems.name[basins[i]]))\n",
    "    axins = inset_axes(axs[i], width=\"30%\", height=\"30%\", loc=\"upper right\", \n",
    "                       axes_class=cartopy.mpl.geoaxes.GeoAxes, \n",
    "                       bbox_to_anchor=(0.1,-0.025,1,1),bbox_transform=axs[i].transAxes,\n",
    "                       axes_kwargs=dict(projection=ccrs.SouthPolarStereo()))\n",
    "    axins.add_feature(cartopy.feature.LAND, color='#B3B3B3',zorder=2,alpha=0.5)\n",
    "    icems.loc[[basins[i]],'geometry'].plot(ax=axins,color='#B3B3B3', linewidth=0.45,zorder=3)\n",
    "    axins.coastlines(resolution='10m',linewidth=0.25,zorder=4)\n",
    "    axins.plot(orig.x[grids[0][1]],orig.y[grids[0][0]], marker='*', color='k',zorder=4)\n",
    "    axs[i].set_ylabel('Melt Rate ($m/yr$)');\n",
    "    axs[i].set_xlabel('');\n",
    "    axs[i].legend(loc=\"lower right\");\n",
    "    axs[i].grid(color='dimgray', linestyle='-', linewidth=0.2)\n",
    "    del rec01, rec1, rec3a\n",
    "\n",
    "axs[3].set_xlabel('Time (months)');"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
