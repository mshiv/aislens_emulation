{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "395c628d-d8ab-4071-9c7d-f0c66ed67011",
   "metadata": {},
   "source": [
    "This notebook explores the ocean model simulation used as an input to the generator.\n",
    "We make use of the Southern Ocean Regionally Refined Mesh v 2.1 run of the E3SM / MPAS-Ocean model, a 1000 year simulation of the ocean circulation, provided at a monthly resolution.\n",
    "The notebook uses `dask` to chunk this dataset throughout the workflow to enable scalable computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2349f859-1873-46d8-b695-506e1b0106aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "os.environ['USE_PYGEOS'] = '0'\n",
    "import gc\n",
    "import collections\n",
    "\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib import rcParams, cycler\n",
    "from matplotlib import animation, rc\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "import geopandas as gpd\n",
    "\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "from xeofs.xarray import EOF\n",
    "import rioxarray\n",
    "\n",
    "import dask\n",
    "import distributed\n",
    "\n",
    "import scipy\n",
    "from scipy import signal\n",
    "import cftime\n",
    "from shapely.geometry import mapping\n",
    "from xarrayutils.utils import linear_trend, xr_linregress\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90b38f2-f027-46ba-b424-96ced44f0877",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = distributed.Client()\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2810b82b-ac42-438e-aa08-7bc76629b7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File path directories\n",
    "\n",
    "inDirName = '/Users/smurugan9/research/aislens/aislens_emulation/'\n",
    "DIR_external = 'data/external/'\n",
    "DIR_interim = 'data/interim/'\n",
    "\n",
    "# DATASET FILEPATHS\n",
    "# Ocean model output - E3SM (SORRMv2.1.ISMF), data received from Darin Comeau / Matt Hoffman at LANL\n",
    "DIR_SORRMv21 = 'data/external/SORRMv2.1.ISMF/regridded_output/'\n",
    "\n",
    "# INTERIM GENERATED FILEPATHS\n",
    "DIR_SORRMv21_Interim = 'data/interim/SORRMv2.1.ISMF/iceShelves_dedraft/'\n",
    "\n",
    "# DATA FILENAMES\n",
    "FILE_SORRMv21 = 'Regridded_SORRMv2.1.ISMF.FULL.nc'\n",
    "\n",
    "# Ice shelf basin/catchment definitions\n",
    "FILE_iceShelvesShape = 'iceShelves.geojson'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780d0a92-42d9-4e52-8cab-bc79c7483d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "SORRMv21 = xr.open_dataset(inDirName+DIR_SORRMv21+FILE_SORRMv21, chunks={\"Time\":36})\n",
    "\n",
    "ICESHELVES_MASK = gpd.read_file(inDirName+DIR_external+FILE_iceShelvesShape)\n",
    "icems = ICESHELVES_MASK.to_crs({'init': 'epsg:3031'});\n",
    "crs = ccrs.SouthPolarStereo();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5ac655-3949-4915-b119-ddf6d6ff2b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "flux = SORRMv21.timeMonthly_avg_landIceFreshwaterFlux\n",
    "# ssh = SORRMv21.timeMonthly_avg_ssh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5fb4a0c-8bcf-43a6-998f-c90397fa7424",
   "metadata": {},
   "source": [
    "### Detrend and deseasonalize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb3e307-05d2-4bb6-b766-5dd5343e9e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear trend, if any\n",
    "# Debug for dask implementation, a \"consistent source of headaches\".\n",
    "# https://ncar.github.io/esds/posts/2022/dask-debug-detrend/\n",
    "\n",
    "def detrend_dim(data, dim, deg):\n",
    "    # detrend along a single dimension\n",
    "    p = data.polyfit(dim=dim, deg=deg)\n",
    "    fit = xr.polyval(data[dim], p.polyfit_coefficients)\n",
    "    return data - fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278964c4-0238-4014-b457-a8ff170b08bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = flux.polyfit(dim='Time',deg=1)\n",
    "p.polyfit_coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1f2f6f-0332-45c8-a1a2-9b87529f014f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit = xr.polyval(flux['Time'], p.polyfit_coefficients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a962d91-5f8d-44d8-a081-cdd309e75a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 'Time'\n",
    "data = flux\n",
    "chunked_dim = xr.DataArray(dask.array.from_array(data[dim].data, chunks=data.chunksizes[dim]), dims=dim, name=dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3244945a-ea59-494a-b3be-37d0f89d05c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# polyval function defined below does not work for a Datetime vector time dimension, only for integers. \n",
    "# TODO: Modify function to convert that to a vector of floats or ints.\n",
    "\n",
    "def polyval(coord, coeffs, degree_dim=\"degree\"):\n",
    "    x = np.array(range(12000))# coord.data\n",
    "\n",
    "    deg_coord = coeffs[degree_dim]\n",
    "    N = int(deg_coord.max()) + 1\n",
    "\n",
    "    lhs = xr.DataArray(\n",
    "        np.stack([x ** (N - 1 - i) for i in range(N)], axis=1),\n",
    "        dims=(coord.name, degree_dim),\n",
    "        coords={coord.name: coord, degree_dim: np.arange(deg_coord.max() + 1)[::-1]},\n",
    "    )\n",
    "    return (lhs * coeffs).sum(degree_dim)\n",
    "\n",
    "\n",
    "# Function to detrend\n",
    "# Modified from source: https://gist.github.com/rabernat/1ea82bb067c3273a6166d1b1f77d490f\n",
    "def detrend_dim(da, dim, deg=1):\n",
    "    \"\"\"detrend along a single dimension.\"\"\"\n",
    "    # calculate polynomial coefficients\n",
    "    p = da.polyfit(dim=dim, deg=deg, skipna=False)\n",
    "    # first create a chunked version of the \"ocean_time\" dimension\n",
    "    chunked_dim = xr.DataArray(dask.array.from_array(da[dim].data, chunks=da.chunksizes[dim]), dims=dim, name=dim)\n",
    "    fit = polyval(chunked_dim, p.polyfit_coefficients)\n",
    "    # evaluate trend\n",
    "    # remove the trend\n",
    "    return da - fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b43820-4920-499c-866e-0bf6ff8d9ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked_dim.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f9f44c-aacd-4052-829d-d2c481160c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "flux_detrend = detrend_dim(flux,\"Time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46346e95-e770-4ffa-b90c-99b37b167ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "flux_detrend_computed = flux_detrend.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94899392-7611-4788-862c-b3fd55cdb606",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e526056-07aa-40ef-9fdd-766ed6c9d98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import scipy.signal as sps\n",
    "import scipy.linalg as spl\n",
    "\n",
    "\n",
    "def detrend(da, dim, detrend_type=\"constant\"):\n",
    "    \"\"\"\n",
    "    Detrend a DataArray\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    da : xarray.DataArray\n",
    "        The data to detrend\n",
    "    dim : str or list\n",
    "        Dimensions along which to apply detrend.\n",
    "        Can be either one dimension or a list with two dimensions.\n",
    "        Higher-dimensional detrending is not supported.\n",
    "        If dask data are passed, the data must be chunked along dim.\n",
    "    detrend_type : {'constant', 'linear'}\n",
    "        If ``constant``, a constant offset will be removed from each dim.\n",
    "        If ``linear``, a linear least-squares fit will be estimated and removed\n",
    "        from the data.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    da : xarray.DataArray\n",
    "        The detrended data.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    This function will act lazily in the presence of dask arrays on the\n",
    "    input.\n",
    "    \"\"\"\n",
    "\n",
    "    if dim is None:\n",
    "        dim = list(da.dims)\n",
    "    else:\n",
    "        if isinstance(dim, str):\n",
    "            dim = [dim]\n",
    "\n",
    "    if detrend_type not in [\"constant\", \"linear\", None]:\n",
    "        raise NotImplementedError(\n",
    "            \"%s is not a valid detrending option. Valid \"\n",
    "            \"options are: 'constant','linear', or None.\" % detrend_type\n",
    "        )\n",
    "\n",
    "    if detrend_type is None:\n",
    "        return da\n",
    "    elif detrend_type == \"constant\":\n",
    "        return da - da.mean(dim=dim)\n",
    "    elif detrend_type == \"linear\":\n",
    "        data = da.data\n",
    "        axis_num = [da.get_axis_num(d) for d in dim]\n",
    "        chunks = getattr(data, \"chunks\", None)\n",
    "        if chunks:\n",
    "            axis_chunks = [data.chunks[a] for a in axis_num]\n",
    "            print(axis_chunks)\n",
    "            if not all([len(ac) == 1 for ac in axis_chunks]):\n",
    "                raise ValueError(\"Contiguous chunks required for detrending.\")\n",
    "        if len(dim) == 1:\n",
    "            dt = xr.apply_ufunc(\n",
    "                sps.detrend,\n",
    "                da,\n",
    "                axis_num[0],\n",
    "                output_dtypes=[da.dtype],\n",
    "                dask=\"parallelized\",\n",
    "            )\n",
    "        elif len(dim) == 2:\n",
    "            dt = xr.apply_ufunc(\n",
    "                _detrend_2d_ufunc,\n",
    "                da,\n",
    "                input_core_dims=[dim],\n",
    "                output_core_dims=[dim],\n",
    "                output_dtypes=[da.dtype],\n",
    "                vectorize=True,\n",
    "                dask=\"parallelized\",\n",
    "            )\n",
    "        else:  # pragma: no cover\n",
    "            raise NotImplementedError(\n",
    "                \"Only 1D and 2D detrending are implemented so far.\"\n",
    "            )\n",
    "\n",
    "    return dt\n",
    "\n",
    "\n",
    "def _detrend_2d_ufunc(arr):\n",
    "    assert arr.ndim == 2\n",
    "    N = arr.shape\n",
    "\n",
    "    col0 = np.ones(N[0] * N[1])\n",
    "    col1 = np.repeat(np.arange(N[0]), N[1]) + 1\n",
    "    col2 = np.tile(np.arange(N[1]), N[0]) + 1\n",
    "    G = np.stack([col0, col1, col2]).transpose()\n",
    "\n",
    "    d_obs = np.reshape(arr, (N[0] * N[1], 1))\n",
    "    m_est = np.dot(np.dot(spl.inv(np.dot(G.T, G)), G.T), d_obs)\n",
    "    d_est = np.dot(G, m_est)\n",
    "    linear_fit = np.reshape(d_est, N)\n",
    "    return arr - linear_fit\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db694391-3c43-4bce-831f-9f23c1ca6d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deseasonalize\n",
    "# Remove climatologies to isolate anomalies / deseasonalize\n",
    "def deseasonalize(data):\n",
    "    data_month = data.groupby(\"Time.month\")\n",
    "    data_clm = data_month.mean(\"Time\") # Climatologies\n",
    "    data_anm = data_month - data_clm # Deseasonalized anomalies\n",
    "    return data_anm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9b76e6-9759-4dc3-8e6c-caf02f2679d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "flux_detrend = detrend_dim(flux,\"Time\",1)\n",
    "# flux_clean = deseasonalize(flux_detrend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726dd04c-e064-4a3b-8d33-f68fded9b927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flux_detrend = flux_detrend.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5b0458-6247-4142-9e2e-6c5ca8693e37",
   "metadata": {},
   "source": [
    "### Temporal Standard Deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd2860f-2b1a-4d69-b4a9-9f9b3f2d9f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard deviation in time\n",
    "flux_std = flux.std('Time').compute()\n",
    "flux_std.where(flux_std!=0).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dddf9b67-c52f-46a5-bbcb-4c1afb5609dd",
   "metadata": {},
   "source": [
    "### Temporal Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982f627f-8412-4f02-8c22-d7b95333d661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time mean\n",
    "flux_tmean = flux.mean('Time').compute()\n",
    "flux_tmean.where(flux_std!=0).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f15f715-2b06-470d-8da4-a3dd6071c758",
   "metadata": {},
   "source": [
    "### Cumulative melt rate (across the ice sheet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2c3294-d93e-48af-a930-f3c2623a93ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "flux_ts = flux.sum(['x','y']).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba14a11a-8bd6-482b-a918-28224b4a029b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25,8))\n",
    "flux_ts.plot()\n",
    "plt.xlabel('Time (Simulation years)')\n",
    "plt.title('Freshwater Flux - AIS Cumulative')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d75235-40f8-4a05-bf3f-11dbfad676ed",
   "metadata": {},
   "source": [
    "### Mean freshwater flux in each catchment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfe22a1-cf79-421b-bb9e-f4f404674e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_data(total_data, basin):\n",
    "    clipped_data = total_data.rio.clip(icems.loc[[basin],'geometry'].apply(mapping))\n",
    "    #clipped_data = clipped_data.dropna('time',how='all')\n",
    "    #clipped_data = clipped_data.dropna('y',how='all')\n",
    "    #clipped_data = clipped_data.dropna('x',how='all')\n",
    "    # clipped_data = clipped_data.drop(\"month\")\n",
    "    return clipped_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905cef23-387a-4282-b07e-cf53adaadff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_flux = np.empty(len(icems[33:133]))\n",
    "\n",
    "for i in range(33,133):\n",
    "    clip_ds = clip_data(flux_tmean, i)\n",
    "    mean_flux[i-33] = clip_ds.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8bd9d33-a30f-486f-8024-66dd4b3d17a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flux_tmean.rio.write_crs(\"epsg:3031\",inplace=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32996ea-182a-4b76-9166-1d902165651d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25,8))\n",
    "plt.plot(mean_flux, marker='x', lw=0.0)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
